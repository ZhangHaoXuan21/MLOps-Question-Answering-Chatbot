{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Test Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This notebook will focus on preparing test cases to evaluate different RAG pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Manually Prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Simple query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_dict = {\n",
    "    \"question\":[],\n",
    "    \"expected_answer\":[],\n",
    "    \"query_type\":[],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_1 = \"\"\"\n",
    "What is data distribution shifts ?\n",
    "\"\"\"\n",
    "\n",
    "answer_question_1 = \"\"\"\n",
    "**Data Distribution Shifts Definition:\n",
    "\n",
    "Data distribution shifts refer to changes in the underlying distribution of the data, which can affect the performance of machine learning models. These shifts can occur in various forms, including concept drift, covariate shift, and label shift.\n",
    "\n",
    "Types of Data Distribution Shifts:\n",
    "1. Covariate Shift: This occurs when the distribution of the input data (P(X)) changes, but the conditional probability of the output given the input (P(Y|X)) remains the same.\n",
    "2. Label Shift: This occurs when the distribution of the output data (P(Y)) changes, but the conditional probability of the input given the output (P(X|Y)) remains the same.\n",
    "3. Concept Drift: This occurs when the conditional probability of the output given the input (P(Y|X)) changes, but the distribution of the input data (P(X)) remains the same.\n",
    "\"\"\"\n",
    "\n",
    "testing_dict['question'].append(question_1)\n",
    "testing_dict['expected_answer'].append(answer_question_1)\n",
    "testing_dict['query_type'].append(\"simple_query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_2 = \"\"\"\n",
    "What is the train-serving skew ?\n",
    "\"\"\"\n",
    "\n",
    "answer_question_2 = \"\"\"\n",
    "Train-Serving Skew Definition and Explanation:\n",
    "\n",
    "The train-serving skew refers to a common failure mode in machine learning (ML) models where a model performs well during development but poorly when deployed in production. This occurs when the training data and the real-world data (also known as unseen data) come from different distributions.\n",
    "\n",
    "Causes of Train-Serving Skew*\n",
    "\n",
    "1. Divergence between training and real-world data: The underlying distribution of real-world data is unlikely to be the same as the underlying distribution of the training data. This can be due to various selection and sampling biases, such as differences in encoding, data types, or other factors.\n",
    "2. Non-stationarity of real-world data: Real-world data is constantly changing, and data distributions shift over time.\n",
    "\"\"\"\n",
    "\n",
    "testing_dict['question'].append(question_2)\n",
    "testing_dict['expected_answer'].append(answer_question_2)\n",
    "testing_dict['query_type'].append(\"simple_query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_3 = \"\"\"\n",
    "What is degenerate feedback loops? Give some examples\n",
    "\"\"\"\n",
    "\n",
    "answer_question_3 = \"\"\"\n",
    "Degenerate Feedback Loops Definition:\n",
    "\n",
    "Degenerate feedback loops occur when a system's outputs are used to generate its future inputs, which in turn influence the system's future outputs. This can lead to unintended consequences, such as perpetuating and magnifying biases embedded in the data.\n",
    "\n",
    "Examples of Degenerate Feedback Loops:\n",
    "1. Recommender Systems: A system recommends songs to users based on their past listening history. The songs that are ranked high by the system are shown first to users, which makes users click on them more. This, in turn, makes the system rank these songs even higher.\n",
    "2. Resume-Screening Model: A model is trained to predict whether a candidate is qualified for a job based on their resume. The model finds that feature X (e.g., \"went to Stanford\") accurately predicts whether someone is qualified, so it recommends resumes with feature X. Recruiters only interview people whose resumes are recommended by the model, which means they only interview candidates with feature X. This, in turn, makes the model put even more weight on feature X.\n",
    "3. Popularity Bias: Popular movies, books, or songs keep getting more popular, which makes it hard for new items to break into popular lists. This is an example of a degenerate feedback loop, where the popularity of an item is used to generate more popularity.\n",
    "\"\"\"\n",
    "\n",
    "testing_dict['question'].append(question_3)\n",
    "testing_dict['expected_answer'].append(answer_question_3)\n",
    "testing_dict['query_type'].append(\"simple_query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_4 = \"\"\"\n",
    "Give me 2 metrics to measure the diversity of the outputs of a recommender system.\n",
    "\"\"\"\n",
    "\n",
    "answer_question_4 = \"\"\"\n",
    "1. Aggregate Diversity: This metric measures the overall diversity of the system's outputs. It takes into account the variety of items recommended to users and the distribution of these items. A higher score indicates a more diverse set of recommendations.\n",
    "\n",
    "2. Average Coverage of Long-Tail Items: This metric measures the extent to which the system recommends items from the long tail of the popularity distribution. The long tail consists of items that are rarely interacted with, but are still relevant to users. A higher score indicates that the system is better at recommending less popular items, which can help to reduce popularity bias.\n",
    "\"\"\"\n",
    "\n",
    "testing_dict['question'].append(question_4)\n",
    "testing_dict['expected_answer'].append(answer_question_4)\n",
    "testing_dict['query_type'].append(\"simple_query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_5 = \"\"\"\n",
    "What is the difference between machine learning in research vs in production ?.\n",
    "\"\"\"\n",
    "\n",
    "answer_question_5 = \"\"\"\n",
    "1. Requirements:\n",
    "-Research: Focuses on achieving state-of-the-art model performance on benchmark datasets.\n",
    "-Production: Different stakeholders have different requirements, which can include accuracy, latency, scalability, and interpretability.\n",
    "\n",
    "2. Computational Priority:\n",
    "Research: Prioritizes fast training and high throughput to quickly experiment with different models and hyperparameters.\n",
    "Production: Prioritizes fast inference and low latency to ensure real-time predictions and efficient processing.\n",
    "\n",
    "3. Data:\n",
    "Research: Data is often clean and well-formated.\n",
    "Production: Data is messy with a lot of noice, unstructured, constantly shifting. Labels, if there are any, might be sparse, imbalanced, or incorrect.\n",
    "\n",
    "4. Fairness:\n",
    "Research: Fairness is not a top consideration.\n",
    "Production: Fairness must be considered when the prediction is conducted on wide variety of populations.\n",
    "\n",
    "5. Interpretability:\n",
    "Research: Interpretability is not a top consideration since research is mainly evaluated on model performance.\n",
    "Production: Interpretability may be considered for both business leaders and end users, to understand why a decision is made so that they can trust a model.\n",
    "\"\"\"\n",
    "\n",
    "testing_dict['question'].append(question_5)\n",
    "testing_dict['expected_answer'].append(answer_question_5)\n",
    "testing_dict['query_type'].append(\"simple_query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_6 = \"\"\"\n",
    "What are the types of software system failures in ML pipelines ?.\n",
    "\"\"\"\n",
    "\n",
    "answer_question_6 = \"\"\"\n",
    "1. Dependency Failure: This type of failure occurs when a software package or codebase that the system depends on breaks, causing the system to fail. This is common when the dependency is maintained by a third party, especially if the third party no longer exists.\n",
    "\n",
    "2. Deployment Failure: Deployment failures occur due to errors during deployment, such as deploying an older version of the model instead of the current version, or when the system lacks the necessary permissions to read or write certain files.\n",
    "\n",
    "3. Hardware Failures: Hardware failures occur when the hardware used to deploy the model, such as CPUs or GPUs, does not behave as expected. For example, the CPUs may overheat and break down.\n",
    "\n",
    "4. Downtime or Crashing: This type of failure occurs when a component of the system, such as a server, is down, causing the entire system to be unavailable.\n",
    "\"\"\"\n",
    "\n",
    "testing_dict['question'].append(question_6)\n",
    "testing_dict['expected_answer'].append(answer_question_6)\n",
    "testing_dict['query_type'].append(\"simple_query\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Complex query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_7 = \"\"\"\n",
    "What is data distribution shifts ? how it happen ? how to detect? how to solve it ?.\n",
    "\"\"\"\n",
    "\n",
    "answer_question_7 = \"\"\"\n",
    "QUESTION: What is Data Distribution Shift?\n",
    "ANSWER:\n",
    "Data distribution shift refers to the change in the underlying distribution of the data, which can affect the performance of a machine learning model. It is often used interchangeably with concept drift, covariate shift, and label shift, but these are distinct subtypes of data shift.\n",
    "\n",
    "Types of Data Distribution Shifts:\n",
    "\n",
    "1. Covariate Shift: When the distribution of the input (X) changes, but the conditional probability of an output given an input (P(Y|X)) remains the same.\n",
    "2. Label Shift: When the distribution of the output (Y) changes, but the conditional probability of an input given an output (P(X|Y)) remains the same.\n",
    "3. Concept Drift: When the conditional probability of an output given an input (P(Y|X)) changes, but the distribution of the input (X) remains the same.\n",
    "\n",
    "QUESTION: How Data Distribution Shift Happen?\n",
    "ANSWER:\n",
    "Data distribution shift can occur due to various reasons, including:\n",
    "\n",
    "1. Biases in data selection: This can result in an unrepresentative sample of the population, leading to covariate shift.\n",
    "2. Changes in the environment: This can cause concept drift, where the underlying relationships between the input and output variables change.\n",
    "\n",
    "QUESTION: How to Detect Data Distribution Shift?\n",
    "ANSWER:\n",
    "Detecting data distribution shift can be challenging, but some methods include:\n",
    "\n",
    "1. Monitoring accuracy-related metrics: This can help identify changes in the model's performance.\n",
    "2. Monitoring input distribution: This can help identify changes in the distribution of the input data.\n",
    "3. Monitoring label distribution: This can help identify changes in the distribution of the output data.\n",
    "4. Using techniques such as Black Box Shift Estimation: This can help detect label shifts without requiring labels from the target distribution.\n",
    "\n",
    "QUESTION: How to Solve Data Distribution Shift?\n",
    "ANSWER:\n",
    "1. Updating models: This can involve retraining the model from scratch (stateless retraining) or continue training it from the last checkpoint (stateful training).\n",
    "2. Monitoring and observability: This can help identify changes in the data distribution and trigger updates to the model.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "testing_dict['question'].append(question_7)\n",
    "testing_dict['expected_answer'].append(answer_question_7)\n",
    "testing_dict['query_type'].append(\"complex_query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_8 = \"\"\"\n",
    "Can latency happen if my feature engineering contain too many features ?\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "answer_question_8 = \"\"\"\n",
    "Yes, having too many features can lead to increased latency in your model. This is particularly true when it comes to online prediction, where features need to be extracted from raw data in real-time.\n",
    "\n",
    "There are several reasons why an excessive number of features can contribute to higher latency:\n",
    "\n",
    "1. Increased Computational Requirements: More features require more computational resources to process, which can lead to slower inference times and increased latency.\n",
    "2. Data Extraction and Processing: When features need to be extracted from raw data in real-time, having too many features can slow down the process, leading to increased latency.\n",
    "3. Model Complexity: Models with too many features can become overly complex, leading to slower inference times and increased latency.\n",
    "\"\"\"\n",
    "\n",
    "testing_dict['question'].append(question_8)\n",
    "testing_dict['expected_answer'].append(answer_question_8)\n",
    "testing_dict['query_type'].append(\"complex_query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_9 = \"\"\"\n",
    "What is data leakage in mlops context? how it happens, how to detect and what are the solutions to it ?\n",
    "\"\"\"\n",
    "\n",
    "answer_question_9 = \"\"\" \n",
    "QUESTION: What is Data Leakage in MLOps Context?\n",
    "ANSWER:\n",
    "Data leakage refers to the phenomenon when a form of the label \"leaks\" into the set of features used for making predictions, and this same information is not available during inference. This can cause machine learning models to fail in unexpected and spectacular ways, even after extensive evaluation and testing.\n",
    "\n",
    "QUESTION: How Does Data Leakage Happen ?\n",
    "ANSWER:\n",
    "1. Splitting time-correlated data randomly: When data is time-correlated, splitting it randomly can cause information from the future to leak into the training process.\n",
    "2. Scaling and normalization: Using statistics from the entire dataset to scale and normalize features can cause leakage.\n",
    "3. Filling in missing data: Filling in missing data with statistics from the test split can cause leakage.\n",
    "4. Poor handling of data duplication: Failing to remove duplicates before splitting data can cause the same samples to appear in both train and validation/test splits.\n",
    "5. Group leakage: A group of examples with strongly correlated labels can be divided into different splits, causing leakage.\n",
    "\n",
    "QUESTION: How to Detect Data Leakage?\n",
    "ANSWER:\n",
    "1. Measure the predictive power of each feature or a set of features: If a feature has unusually high correlation with the target variable, investigate how this feature is generated and whether the correlation makes sense.\n",
    "2. Keep track of the sources of your data and understand how it is collected and processed: This can help identify potential sources of leakage.\n",
    "Detecting data leakage requires a deep understanding of the way data is collected and processed. Some ways to detect data leakage include:\n",
    "\n",
    "QUESTION: What are the solutions to Data Leakage?\n",
    "ANSWER:\n",
    "1. Split data by time: When data is time-correlated, split it by time to avoid leakage.\n",
    "2. Use only statistics from the train split to scale and fill in missing values: This can prevent leakage caused by scaling and filling in missing values.\n",
    "3. Remove duplicates before splitting: Check for duplicates before splitting and also after splitting to ensure that the same samples do not appear in both train and validation/test splits.\n",
    "4. Normalize data: Normalize data so that data from different sources can have the same means and variances.\n",
    "5. Incorporate subject matter experts: Incorporate subject matter experts into the ML design process to gain a deeper understanding of how data is collected and processed.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "testing_dict['question'].append(question_9)\n",
    "testing_dict['expected_answer'].append(answer_question_9)\n",
    "testing_dict['query_type'].append(\"complex_query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_10 = \"\"\" \n",
    "Can I just simply replace my existing model with my new retrained model ?\n",
    "\"\"\"\n",
    "\n",
    "answer_question_10 = \"\"\"\n",
    "Replacing an Existing Model with a New Retrained Model: Considerations and Best Practices:\n",
    "\n",
    "No, you should not simply replace an existing model with a new retrained model. While it may seem like a simple swap, there are several factors to consider before making the change.\n",
    "\n",
    "1. Retraining Strategies: There are two primary retraining strategies to consider: stateless retraining (training from scratch) and stateful training (fine-tuning). The choice between these two approaches depends on the specific use case and the characteristics of the data.\n",
    "2. Data Considerations: When retraining a model, it's essential to consider the data used for retraining. This includes deciding which data to use (e.g., data from the last 24 hours, last week, last 6 months, or from the point when data has started to drift). The choice of data will impact the performance of the retrained model.\n",
    "3. Continual Learning: Continual learning is a training paradigm where a model updates itself with every incoming sample in production. However, this approach can be challenging to implement, and most companies update their models in micro-batches instead.\n",
    "\n",
    "Best Practices for Replacing an Existing Model:\n",
    "1. Evaluate the retrained model: Ensure that the retrained model performs better than the existing model before deploying it.\n",
    "2. Use a champion-challenger approach: Create a replica of the existing model and update this replica on new data. Only replace the existing model with the updated replica if the updated replica proves to be better.\n",
    "3. Consider the retraining frequency: Determine the optimal retraining frequency for each model, taking into account the specific needs of each model.\n",
    "\"\"\"\n",
    "\n",
    "testing_dict['question'].append(question_10)\n",
    "testing_dict['expected_answer'].append(answer_question_10)\n",
    "testing_dict['query_type'].append(\"complex_query\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Use Langchain QAGenerateChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation.qa import QAGenerateChain\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare llm\n",
    "groq_llama3_1_70b = ChatGroq(\n",
    "    model=\"llama-3.1-70b-versatile\",\n",
    "    temperature=0,\n",
    "    api_key=config.GROQ_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PDFs Start.\n",
      "Loading PDFs End.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 1. Load multiple PDFs using PyPDFLoader, only if the file is a PDF\n",
    "# -------------------------------------------------------------------------\n",
    "print(\"Loading PDFs Start.\")\n",
    "\n",
    "file_paths = config.PDF_FILE_PATHS\n",
    "docs = []\n",
    "\n",
    "for file_path in file_paths:\n",
    "    # Check if the file is a PDF\n",
    "    if file_path.lower().endswith(\".pdf\"):\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        docs.extend(loader.load())  # Combine all loaded docs into a single list\n",
    "    else:\n",
    "        print(f\"Skipping non-PDF file: {file_path}\")\n",
    "\n",
    "print(\"Loading PDFs End.\")\n",
    "\n",
    "# Preprocess page content\n",
    "for doc in docs:\n",
    "    # Replace tabs with spaces in each document's page_content\n",
    "    doc.page_content = doc.page_content.replace(\"\\t\", \" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(1024)\n",
    "\n",
    "random_docs = list(np.random.choice(docs, size=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '../designing-machine-learning-systems.pdf', 'page': 443}, page_content='LFs (labeling functions)\\n, \\nWeak supervision\\nheuristics\\n, \\nWeak supervision\\nlogs\\n, \\nData Sources\\n, \\nData Sources\\nexperiment tracking\\n, \\nExperiment tracking\\nmonitoring and\\n, \\nLogs\\n-\\nLogs\\nstorage\\n, \\nData Sources\\nloop tiling, model optimization\\n, \\nModel optimization\\nloss curve\\n, \\nExperiment tracking\\nloss functions\\n, \\nObjective Functions\\n(\\nsee also\\n objective functions)\\nlow-rank factorization\\n, \\nLow-Rank Factorization\\n-\\nLow-Rank Factorization\\nM\\nmaintainability\\n, \\nMaintainability\\nManning, Christopher\\n, \\nMind Versus Data\\nMAR (missing at random) values\\n, \\nHandling Missing Values\\nMCAR (missing completely at random) values\\n, \\nHandling Missing Values\\nmerge conflicts\\n, \\nVersioning\\nmessage queue, dataflow and\\n, \\nData Passing Through Real-Time Transport\\nMetaflow\\n, \\nData Science Workflow Management\\nmetrics\\nmonitoring and\\n, \\nMonitoring and Observability\\naccuracy-related metrics\\n, \\nMonitoring accuracy-related metrics\\nfeatures\\n, \\nMonitoring features\\n-\\nMonitoring features'),\n",
       " Document(metadata={'source': '../designing-machine-learning-systems.pdf', 'page': 401}, page_content='However, as I learned more about low-level infrastructure, I realized how\\nunreasonable it is to expect data scientists to know about it. Infrastructure\\nrequires a very different set of skills from data science. In theory, you can learn\\nboth sets of skills. In practice, the more time you spend on one means the less\\ntime you spend on the other. I love Erik Bernhardsson’s analogy that expecting\\ndata scientists to know about infrastructure is like expecting app developers to\\nknow about how Linux kernels work.\\n I joined an ML company because I\\nwanted to spend more time with data, not with spinning up AWS instances,\\nwriting Dockerfiles, scheduling/scaling clusters, or debugging YAML\\nconfiguration files.\\nFor data scientists to own the entire process, we need good tools. In other words,\\nwe need good infrastructure.\\nWhat if we have an abstraction to allow data scientists to own the process end-\\nto-end without having to worry about infrastructure?\\nWhat if I can just tell this tool, “Here’s where I store my data (S3), here are the\\nsteps to run my code (featurizing, modeling), here’s where my code should run\\n(EC2 instances, serverless stuff like AWS Batch, Function, etc.), here’s what my\\ncode needs to run at each step (dependencies),” and then this tool manages all\\nthe infrastructure stuff for me?\\nAccording to both Stitch Fix and Netflix, the success of a full-stack data scientist\\nrelies on the tools they have. They need tools that “abstract the data scientists\\nfrom the complexities of containerization, distributed processing, automatic\\nfailover, and other advanced computer science concepts.”\\nIn Netflix’s model, the specialists—people who originally owned a part of the\\nproject—\\nfirst create tools that automate their parts, as shown in \\nFigure 11-3\\n.\\nData scientists can leverage these tools to own their projects end-to-end.\\n4\\n5'),\n",
       " Document(metadata={'source': '../designing-machine-learning-systems.pdf', 'page': 97}, page_content='independently of one another. Structuring an application as separate services\\ngives you a microservice architecture.\\nTo put the microservice architecture in the context of ML systems, imagine\\nyou’re an ML engineer working on the price optimization problem for a\\ncompany that owns a ride-sharing application like Lyft. In reality, Lyft has\\nhundreds of services\\n in its microservice architecture, but for the sake of\\nsimplicity, let’s consider only \\nthree services:\\nDriver management service\\nPredicts how many drivers will be available in the next minute in a given\\narea.\\nRide management service\\nPredicts how many rides will be requested in the next minute in a given area.\\nPrice optimization service\\nPredicts the optimal price for each ride. The price for a ride should be low\\nenough for riders to be willing to pay, yet high enough for drivers to be\\nwilling to drive and for the company to make a profit.\\nBecause the price depends on supply (the available drivers) and demand (the\\nrequested rides), the price optimization service needs data from both the driver\\nmanagement and ride management services. Each time a user requests a ride, the\\nprice optimization service requests the predicted number of rides and predicted\\nnumber of drivers to predict the optimal price for this ride.\\nThe most popular styles of requests used \\nfor passing data through networks are\\nREST (representational state transfer) and RPC (remote procedure call). Their\\ndetailed analysis is beyond the scope of this book, but one major difference is\\nthat REST was designed for requests over networks, whereas RPC “tries to make\\na request to a remote network service look the same as calling a function or\\nmethod in your programming language\\n.\\n” Because of this, “REST seems to be the\\npredominant style for public APIs. The main focus of RPC frameworks is on\\nrequests between services owned by the same organization, typically within the\\nsame data center.”\\n24\\n25'),\n",
       " Document(metadata={'source': '../designing-machine-learning-systems.pdf', 'page': 101}, page_content='Figure 3-11. \\nCompanies that use Apache Kafka and RabbitMQ. Source: Screenshot \\nfrom \\nStackshare\\nBatch Processing Versus Stream Processing\\nOnce your data arrives in data storage engines like databases, data lakes, or data\\nwarehouses, it becomes historical \\ndata. This is opposed to streaming data (data\\nthat is still streaming in). Historical data is often processed in batch jobs—jobs\\nthat are kicked off periodically. For example, once a day, you might want to kick\\noff a batch job to compute the average surge charge for all the rides in the last\\nday.\\nWhen data is processed in batch jobs, we refer to it as \\nbatch processing\\n. Batch\\nprocessing has been a research subject for many decades, and companies have\\ncome up with distributed systems like MapReduce and Spark to process batch\\ndata efficiently.\\nWhen you have data in real-time transports like Apache Kafka and Amazon\\nKinesis, we say that you have streaming data. \\nStream processing\\n refers to doing\\ncomputation on streaming data. Computation on streaming data can also be\\nkicked off periodically, but the periods are \\nusually much shorter than the periods\\nfor batch jobs (e.g., every five minutes instead of every day). Computation on\\nstreaming data can also be kicked off whenever the need arises. For example,\\nwhenever a user requests a ride, you process your data stream to see what drivers\\nare currently available.\\nStream processing, when done right, can give low latency because you can\\nprocess data as soon as data is generated, without having to first write it into\\ndatabases. Many people believe that stream processing is less efficient than\\nbatch processing because you can’t leverage tools like MapReduce or Spark.\\nThis is not always the case, for two reasons. First, streaming technologies like'),\n",
       " Document(metadata={'source': '../designing-machine-learning-systems.pdf', 'page': 89}, page_content='average age of people in the database, all you have to do is to extract all the age\\nvalues and average them out.\\nThe disadvantage of structured data is that you have to commit your data to a\\npredefined schema. If your schema changes, you’ll have to retrospectively\\nupdate all your data, often causing mysterious bugs in the process. For example,\\nyou’ve never kept your users’ email addresses before but now you do, so you\\nhave to retrospectively update email information to all previous users. One of the\\nstrangest bugs one of my colleagues encountered was when they could no longer\\nuse users’ ages with their transactions, and their data schema replaced all the null\\nages with 0, and their ML model thought the transactions were made by people 0\\nyears old.\\nBecause business requirements change over time, committing to a predefined\\ndata schema can become too restricting. Or you might have data from multiple\\ndata sources that are beyond your control, and it’s impossible to make them\\nfollow the same schema. This is where unstructured data becomes appealing.\\nUnstructured data doesn’t adhere to a predefined data schema. It’s usually text\\nbut can also be numbers, dates, images, audio, etc. For example, a text file of\\nlogs generated by your ML model is unstructured data.\\nEven though unstructured data doesn’t adhere to a schema, it might still contain\\nintrinsic patterns that help you extract structures. For example, the following text\\nis unstructured, but you can notice the pattern that each line contains two values\\nseparated by a comma, the first value is textual, and the second value is\\nnumerical. However, there is no guarantee that all lines must follow this format.\\nYou can add a new line to that text even if that line doesn’t follow this format.\\nLisa, 43\\nJack, 23\\nHuyen, 59\\nUnstructured data also allows for more flexible storage options. For example, if\\nyour storage follows a schema, you can only store data following that schema.\\nBut if your storage doesn’t follow a schema, you can store any type of data. You\\ncan convert all your data, regardless of types and formats, into bytestrings and\\nstore them together.\\nA repository for storing structured data is called a data warehouse. A repository\\n18'),\n",
       " Document(metadata={'source': '../designing-machine-learning-systems.pdf', 'page': 353}, page_content='cores might be joined together to form a \\nlarger compute unit to execute a larger\\njob. A compute unit can be created for a specific short-lived job such as an AWS\\nStep Function or a GCP Cloud Run—the unit will be eliminated after the job\\nfinishes. A compute unit can also be created to be more “permanent,” aka\\nwithout being tied to a job, like a virtual machine. A more permanent compute\\nunit is sometimes called an “instance.”\\nHowever, the compute layer doesn’t always use threads or cores as compute\\nunits. There are compute layers that abstract away the notions of cores and use\\nother units of computation. For example, computation engines like Spark and\\nRay use “job” as their unit, and Kubernetes uses “pod,” a wrapper around\\ncontainers, as its smallest deployable unit. While you can have multiple\\ncontainers in a pod, you can’t independently start or stop different containers in\\nthe same pod.\\nTo execute a job, you first need to load the required data into your compute\\nunit’s memory, then execute the required operations—addition, multiplication,\\ndivision, convolution, etc.—on that data. For example, to add two arrays, you\\nwill first need to load these two arrays into memory, and then perform addition\\non the two arrays. If the compute unit doesn’t have enough memory to load these\\ntwo arrays, the operation will be impossible without an algorithm to handle out-\\nof-memory computation. Therefore, a compute unit is mainly characterized by\\ntwo metrics: how much memory it has and how fast it runs an operation.\\nThe memory metric can be specified using units like GB, and it’s generally\\nstraightforward to evaluate: a compute unit with 8 GB of memory can handle\\nmore data in memory than a compute unit with only 2 GB, and it is generally\\nmore expensive.\\n Some companies care not only how much memory a compute\\nunit has but also how fast it is to load data in and out of memory, so some cloud\\nproviders advertise their instances as having “high bandwidth memory” or\\nspecify their instances’ I/O bandwidth.\\nThe operation speed is more contentious. The most common metric is FLOPS—\\nfloating point operations per second. As the \\nname suggests, this metric denotes\\nthe number of float point operations a compute unit can run per second. You\\nmight see a hardware vendor advertising that their GPUs or TPUs or IPUs\\n(intelligence processing units) have teraFLOPS (one trillion FLOPS) or another\\nmassive number of FLOPS.\\n10'),\n",
       " Document(metadata={'source': '../designing-machine-learning-systems.pdf', 'page': 208}, page_content='With data parallelism, each worker has its own copy of \\nthe whole model and\\ndoes all the computation necessary for its copy of the model. Model parallelism\\nis when different components of your model are trained on different machines,\\nas shown in \\nFigure 6-7\\n. For example, machine 0 handles the computation for the\\nfirst two layers while machine 1 handles the next two layers, or some machines\\ncan handle the forward pass while several others handle the backward pass.\\nFigure 6-7. \\nData parallelism and model parallelism. Source: Adapted from an image by Jure Leskovec\\nModel parallelism can be misleading because in some cases parallelism doesn’t\\nmean that different parts of the model in different machines are executed in\\nparallel. For example, if your model is a massive matrix and the matrix is split\\ninto two halves on two machines, then these two halves might be executed in\\nparallel. However, if your model is a neural network and you put the first layer\\non machine 1 and the second layer on machine 2, and layer 2 needs outputs from\\nlayer 1 to execute, then machine 2 has to wait for machine 1 to finish first to run.\\nPipeline parallelism\\n is a clever technique to make different components of a\\n21'),\n",
       " Document(metadata={'source': '../designing-machine-learning-systems.pdf', 'page': 333}, page_content='This is a simple example to illustrate how the data freshness experiment works.\\nIn practice, you might want your experiments to be much more fine-grained,\\noperating not in months but in weeks, days, even hours or minutes. In 2014,\\nFacebook did a similar experiment for ad click-through-rate prediction and\\nfound out that they could reduce the model’s loss by 1% by going from\\nretraining weekly to retraining daily, and this performance gain was significant\\nenough for them to switch their retraining pipeline from weekly to daily.\\n Given\\nthat online contents today are so much more diverse and users’ attention online\\nchanges much faster, we can imagine that the value of data freshness for ad\\nclick-through \\nrate is even higher. Some of the companies with sophisticated ML\\ninfrastructure have found enough performance gain to switch their retraining\\npipeline to every few minutes.\\nModel iteration versus data iteration\\nWe discussed earlier in this chapter that not all model updates are the same. We\\ndifferentiated between model \\niteration (adding a new feature to an existing\\nmodel architecture or changing the model architecture) and data iteration (same\\nmodel architecture and features but you refresh this model with new data). You\\nmight wonder not only how often to update your model, but also what kind of\\nmodel updates to perform.\\nIn theory, you can do both types of updates, and in practice, you should do both\\nfrom time to time. However, the more resources you spend in one approach, the\\nfewer resources you can spend in another.\\nOn the one hand, if you find that iterating on your data doesn’t give you much\\nperformance gain, then you should spend your resources on finding a better\\nmodel. On the other hand, if finding a better model architecture requires 100X\\ncompute for training and gives you 1% performance whereas updating the same\\nmodel on data from the last three hours requires only 1X compute and also gives\\n1% performance gain, you’ll be better off iterating on data.\\nMaybe in the near future, we’ll get more theoretical understanding to know in\\nwhat situation an approach will work better (cue “call for research”), but as of\\ntoday, no book can give you the answer on which approach will work better for\\nyour specific model on your specific task. You’ll have to do experiments to find\\nout.\\n25\\n26'),\n",
       " Document(metadata={'source': '../designing-machine-learning-systems.pdf', 'page': 32}, page_content='competitions, including the famed $1 million Netflix Prize, and yet it’s not\\nwidely used in production. Ensembling combines “multiple learning algorithms\\nto obtain better predictive performance than could be obtained from any of the\\nconstituent learning algorithms alone.”\\n While it can give your ML system a\\nsmall performance improvement, ensembling tends to make a system too\\ncomplex to be useful in production, e.g., slower to make predictions or harder to\\ninterpret the results. We’ll discuss ensembling further in the section\\n“Ensembles”\\n.\\nFor many tasks, a small improvement in performance can result in a huge boost\\nin revenue or cost savings. For example, a 0.2% improvement in the click-\\nthrough rate for a product recommender system can result in millions of dollars\\nincrease in revenue for an ecommerce site. However, for many tasks, a small\\nimprovement might not be noticeable for users. For the second type of task, if a\\nsimple model can do a reasonable job, complex models must perform\\nsignificantly better to justify the complexity.\\nCRITICISM OF ML LEADERBOARDS\\nIn recent years, there have been many critics of ML leaderboards, both\\ncompetitions such as Kaggle and research leaderboards such as ImageNet or\\nGLUE.\\nAn obvious argument is that in these competitions many of the hard steps\\nneeded for building ML systems are already done for you.\\nA less obvious argument is that due to the multiple-hypothesis testing\\nscenario that happens when you have multiple teams testing on the same\\nhold-out test set, a model can do better than the rest just by chance.\\nThe misalignment of interests between research and production has been\\nnoticed by researchers. In an EMNLP 2020 paper, Ethayarajh and Jurafsky\\nargued that benchmarks have helped drive advances in natural language\\nprocessing (NLP) by incentivizing the creation of more accurate models at\\nthe expense of other qualities valued by practitioners such as compactness,\\nfairness, and energy efficiency.\\nComputational priorities\\n15\\n16\\n17\\n18'),\n",
       " Document(metadata={'source': '../designing-machine-learning-systems.pdf', 'page': 323}, page_content='recommendation and label this recommendation as a good recommendation, as\\nshown in \\nFigure 9-4\\n.\\nFigure 9-4. \\nA simplification of the process of extracting labels from user feedback\\nThe process of looking back into the logs to \\nextract labels is called label\\ncomputation. It can be quite costly if the number of logs is large. Label\\ncomputation can be done with batch processing: e.g., waiting for logs to be\\ndeposited into data warehouses first before running a batch job to extract all\\nlabels from logs at once. However, as discussed previously, this means that we’d\\nneed to wait for data to be deposited first, then wait for the next batch job to run.\\nA much faster approach would be to leverage stream processing to extract labels\\nfrom the real-time transports directly.\\nIf your model’s speed iteration is bottlenecked by labeling speed, it’s also\\npossible to speed up the labeling process by leveraging programmatic labeling\\ntools like Snorkel to generate fast labels with minimal human intervention. It\\nmight also be possible to leverage crowdsourced labels to quickly annotate fresh\\ndata.\\nGiven that tooling around streaming is still nascent, architecting an efficient\\nstreaming-first infrastructure for accessing fresh data and extracting fast labels\\nfrom real-time transports can be engineering-intensive and costly. The good\\nnews is that tooling around streaming is growing fast. Confluent, the platform\\nbuilt on top of Kafka, is a $16 billion company as of October 2021. In late 2020,\\nSnowflake started a team focusing on streaming.\\n As of September 2021,\\nMaterialize has raised $100 million to develop a streaming SQL database.\\n As\\ntooling around streaming matures, it’ll be much \\neasier and cheaper for\\ncompanies to develop a streaming-first infrastructure for ML.\\nEvaluation challenge\\n13\\n14\\n15')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jthxc\\anaconda3\\envs\\jaredllm\\Lib\\site-packages\\langchain\\chains\\llm.py:369: UserWarning: The apply_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "question_answer_chain = QAGenerateChain.from_llm(groq_llama3_1_70b)\n",
    "llm_generated_cases = question_answer_chain.apply_and_parse(\n",
    "    [{\"doc\": t} for t in random_docs]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'qa_pairs': {'query': 'What type of values are described as being missing due to a reason that is related to the observed data, but not to the unobserved data?',\n",
       "   'answer': 'MAR (missing at random) values.'}},\n",
       " {'qa_pairs': {'query': 'According to the text, what is the analogy Erik Bernhardsson uses to describe the expectation that data scientists should know about infrastructure, and what does it compare this expectation to?',\n",
       "   'answer': \"Erik Bernhardsson's analogy compares the expectation that data scientists should know about infrastructure to expecting app developers to know about how Linux kernels work.\"}},\n",
       " {'qa_pairs': {'query': 'In the context of a ride-sharing application like Lyft, what are the three services considered in a simplified microservice architecture, and what is the primary function of each service?',\n",
       "   'answer': 'The three services are: '}},\n",
       " {'qa_pairs': {'query': 'What is the primary difference between batch processing and stream processing in terms of when computation is kicked off, and what is an example of a scenario where stream processing is used?',\n",
       "   'answer': 'The primary difference between batch processing and stream processing is that batch processing is typically kicked off periodically with longer periods (e.g., once a day), whereas stream processing is kicked off with shorter periods (e.g., every five minutes) or whenever the need arises. An example of a scenario where stream processing is used is when a user requests a ride, and the system processes the data stream to see what drivers are currently available.'}},\n",
       " {'qa_pairs': {'query': 'What is one of the potential issues that can arise when retrospectively updating a predefined data schema in a structured data system, and what is an example of a strange bug that can occur as a result?',\n",
       "   'answer': 'One potential issue is that the updated schema may replace null values with default values, such as 0, which can cause errors in machine learning models. For example, a colleague encountered a bug where a machine learning model thought transactions were made by people 0 years old because the updated schema replaced null ages with 0.'}},\n",
       " {'qa_pairs': {'query': 'What are the two main metrics used to characterize a compute unit, and what units are typically used to specify the memory metric?',\n",
       "   'answer': 'A compute unit is mainly characterized by two metrics: how much memory it has and how fast it runs an operation. The memory metric can be specified using units like GB (gigabytes).'}},\n",
       " {'qa_pairs': {'query': 'In model parallelism, what happens when the second layer of a neural network is placed on machine 2 and it requires the outputs from the first layer, which is placed on machine 1, to execute?',\n",
       "   'answer': 'Machine 2 has to wait for machine 1 to finish first to run.'}},\n",
       " {'qa_pairs': {'query': 'According to the text, what was the performance gain Facebook observed when they switched their retraining pipeline for ad click-through-rate prediction from weekly to daily, and what decision did they make as a result?',\n",
       "   'answer': \"Facebook observed a 1% reduction in the model's loss when they switched their retraining pipeline from weekly to daily, and this performance gain was significant enough for them to switch their retraining pipeline from weekly to daily.\"}},\n",
       " {'qa_pairs': {'query': 'According to the text, what is one potential issue with ensembling in machine learning systems, and what is the trade-off for the potential performance improvement it offers?',\n",
       "   'answer': 'One potential issue with ensembling is that it can make a system too complex to be useful in production, resulting in slower prediction times or harder-to-interpret results. The trade-off for this complexity is a small performance improvement, which may be justified in certain tasks where even a small improvement can result in significant revenue or cost savings.'}},\n",
       " {'qa_pairs': {'query': \"What are two alternative methods mentioned in the text for speeding up the labeling process when a model's speed iteration is bottlenecked by labeling speed?\",\n",
       "   'answer': 'The two alternative methods mentioned are leveraging programmatic labeling tools like Snorkel to generate fast labels with minimal human intervention, and leveraging crowdsourced labels to quickly annotate fresh data.'}}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_generated_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for case in llm_generated_cases:\n",
    "    question = case['qa_pairs']['query']\n",
    "    expect_answer = case['qa_pairs']['answer']\n",
    "\n",
    "    testing_dict['question'].append(question)\n",
    "    testing_dict['expected_answer'].append(expect_answer)\n",
    "    testing_dict['query_type'].append(\"llm_generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final: Save the test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test_cases.pkl']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(value=testing_dict, filename=\"test_cases.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jaredllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
